{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10aa15f",
   "metadata": {},
   "source": [
    "\n",
    "# TensorFlow Training Pipeline (MobileNetV2 / EfficientNetV2) — Mirror of PyTorch Notebook\n",
    "\n",
    "This notebook mirrors the structure of your original PyTorch pipeline but implements it with **TensorFlow/Keras**:\n",
    "- GPU setup & memory\n",
    "- Data input (tf.data) with **Keras preprocessing** augmentations\n",
    "- Transfer learning with **MobileNetV2** and **EfficientNetV2**\n",
    "- Freeze → Gradual unfreeze (incremental sessions)\n",
    "- Training loops with callbacks (checkpointing best model)\n",
    "- Metrics, confusion matrix, classification report\n",
    "- Single-image inference utility\n",
    "- History logging to CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4159dba2",
   "metadata": {},
   "source": [
    "## 1) Environment & GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, time, sys, platform\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs:\", gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"Enabled memory growth. Logical GPUs: {logical}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU detected — training will run on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c105a",
   "metadata": {},
   "source": [
    "## 2) Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f43838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    train_dir: str = \"datasets/train\"\n",
    "    val_dir: str = \"datasets/val\"\n",
    "    test_dir: str = \"datasets/test\"\n",
    "    image_size: tuple = (224, 224)\n",
    "    batch_train: int = 128\n",
    "    batch_val: int = 64\n",
    "    batch_test: int = 32\n",
    "    num_classes: int = 6  # adjust to your dataset\n",
    "    seed: int = 42\n",
    "    base_learning_rate: float = 1e-4\n",
    "    mixed_precision: bool = True\n",
    "    model_dir: str = \"models_tf\"\n",
    "    best_model_name: str = \"best_model.keras\"\n",
    "    history_csv: str = \"history_log.csv\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Enable mixed precision if GPU is present\n",
    "if tf.config.list_physical_devices('GPU') and cfg.mixed_precision:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision as mp\n",
    "        mp.set_global_policy('mixed_float16')\n",
    "        print(\"Mixed precision enabled.\")\n",
    "    except Exception as e:\n",
    "        print(\"Mixed precision not available:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6733f9",
   "metadata": {},
   "source": [
    "## 3) Data: tf.data Datasets + Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674429a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Training-time augmentation (applies only during training)\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(0.05, 0.05),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.GaussianNoise(0.05),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "def build_datasets(cfg: Config):\n",
    "    train_ds = keras.utils.image_dataset_from_directory(\n",
    "        cfg.train_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=cfg.image_size,\n",
    "        batch_size=cfg.batch_train,\n",
    "        shuffle=True,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    val_ds = keras.utils.image_dataset_from_directory(\n",
    "        cfg.val_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=cfg.image_size,\n",
    "        batch_size=cfg.batch_val,\n",
    "        shuffle=False,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    test_ds = keras.utils.image_dataset_from_directory(\n",
    "        cfg.test_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        image_size=cfg.image_size,\n",
    "        batch_size=cfg.batch_test,\n",
    "        shuffle=False,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "\n",
    "    class_names = train_ds.class_names\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    def preprocess(images, labels):\n",
    "        images = tf.cast(images, tf.float32) / 255.0\n",
    "        return images, labels\n",
    "\n",
    "    train_ds = (train_ds\n",
    "                .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "                .map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)\n",
    "                .prefetch(AUTOTUNE))\n",
    "\n",
    "    val_ds = val_ds.map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "    test_ds = test_ds.map(preprocess, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds, class_names\n",
    "\n",
    "train_ds, val_ds, test_ds, class_names = build_datasets(cfg)\n",
    "cfg.num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4691ea13",
   "metadata": {},
   "source": [
    "## 4) Models: MobileNetV2 and EfficientNetV2 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2cf89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def build_backbone(name: str, input_shape):\n",
    "    inputs = keras.Input(shape=input_shape + (3,))\n",
    "    x = layers.Resizing(input_shape[0], input_shape[1])(inputs)\n",
    "    if name.lower() in [\"mobilenetv2\", \"mobilenet_v2\"]:\n",
    "        base = keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=x, pooling=\"avg\")\n",
    "    elif name.lower() in [\"efficientnetv2m\", \"efficientnetv2-m\", \"efficientnet_v2_m\"]:\n",
    "        base = keras.applications.EfficientNetV2M(include_top=False, weights=\"imagenet\", input_tensor=x, pooling=\"avg\")\n",
    "    elif name.lower() in [\"efficientnetb0\", \"efficientnet-b0\", \"efficientnet_b0\"]:\n",
    "        base = keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", input_tensor=x, pooling=\"avg\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown backbone: \" + name)\n",
    "    return inputs, base\n",
    "\n",
    "def build_classifier(backbone_name=\"efficientnetv2m\", num_classes=6, input_shape=(224,224), dropout=0.5):\n",
    "    inputs, base = build_backbone(backbone_name, input_shape)\n",
    "    base.trainable = False\n",
    "    y = base.output\n",
    "    y = layers.Dropout(dropout)(y)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(y)\n",
    "    model = Model(inputs, outputs, name=f\"{backbone_name}_classifier\")\n",
    "    return model\n",
    "\n",
    "model = build_classifier(\"efficientnetb0\", num_classes=cfg.num_classes, input_shape=cfg.image_size)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb04d866",
   "metadata": {},
   "source": [
    "## 5) Gradual Unfreezing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d4bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "def unfreeze_last_n_layers(model: keras.Model, n: int):\n",
    "    # Attempt to find the backbone (the largest sub-model)\n",
    "    submodels = [l for l in model.layers if isinstance(l, keras.Model)]\n",
    "    backbone = max(submodels, key=lambda m: len(m.layers)) if submodels else model\n",
    "\n",
    "    total = len(backbone.layers)\n",
    "    to_unfreeze = max(0, min(n, total))\n",
    "    for layer in backbone.layers[-to_unfreeze:]:\n",
    "        if isinstance(layer, L.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    print(f\"Unfroze last {to_unfreeze}/{total} layers of backbone: {backbone.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780704e",
   "metadata": {},
   "source": [
    "## 6) Compile & Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.makedirs(cfg.model_dir, exist_ok=True)\n",
    "\n",
    "def compile_model(model, lr):\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(cfg.model_dir, cfg.best_model_name),\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "earlystop_cb = keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True, verbose=1)\n",
    "reducelr_cb = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "compile_model(model, cfg.base_learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1a6b1",
   "metadata": {},
   "source": [
    "## 7) Train & Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b676341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, train_ds, val_ds, epochs, callbacks):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['accuracy'], label='train_acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def save_history_csv(history, path):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    if os.path.exists(path):\n",
    "        old = pd.read_csv(path)\n",
    "        df = pd.concat([old, df], ignore_index=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"Saved history to\", path)\n",
    "\n",
    "history = train_model(model, train_ds, val_ds, epochs=15, callbacks=[checkpoint_cb, earlystop_cb, reducelr_cb])\n",
    "plot_history(history)\n",
    "save_history_csv(history, os.path.join(cfg.model_dir, cfg.history_csv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472495db",
   "metadata": {},
   "source": [
    "## 8) Incremental Training Sessions (Freeze → Unfreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e37ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def incremental_training(model, sessions, train_ds, val_ds):\n",
    "    session_histories = []\n",
    "    for i, sess in enumerate(sessions, start=1):\n",
    "        print(f\"\\n=== Session {i}: {sess} ===\")\n",
    "        unfreeze = sess.get(\"unfreeze_last_n\", 0)\n",
    "        if unfreeze > 0:\n",
    "            unfreeze_last_n_layers(model, unfreeze)\n",
    "        lr = sess.get(\"lr\", cfg.base_learning_rate)\n",
    "        compile_model(model, lr)\n",
    "        epochs = sess.get(\"epochs\", 10)\n",
    "        hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs,\n",
    "                         callbacks=[checkpoint_cb, earlystop_cb, reducelr_cb], verbose=1)\n",
    "        session_histories.append(hist)\n",
    "    return session_histories\n",
    "\n",
    "sessions = [\n",
    "    {\"epochs\": 10, \"lr\": 1e-4, \"unfreeze_last_n\": 0},\n",
    "    {\"epochs\": 15, \"lr\": 5e-5, \"unfreeze_last_n\": 20},\n",
    "    {\"epochs\": 20, \"lr\": 1e-5, \"unfreeze_last_n\": 60},\n",
    "]\n",
    "\n",
    "session_histories = incremental_training(model, sessions, train_ds, val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f940aa",
   "metadata": {},
   "source": [
    "## 9) Evaluation: Test Set, Classification Report & Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_model_path = os.path.join(cfg.model_dir, cfg.best_model_name)\n",
    "print(\"Loading best model from:\", best_model_path)\n",
    "best_model = keras.models.load_model(best_model_path)\n",
    "\n",
    "test_images, test_labels = [], []\n",
    "for batch_x, batch_y in test_ds:\n",
    "    test_images.append(batch_x.numpy())\n",
    "    test_labels.append(batch_y.numpy())\n",
    "test_images = np.concatenate(test_images, axis=0)\n",
    "test_labels = np.concatenate(test_labels, axis=0)\n",
    "\n",
    "probs = best_model.predict(test_images, batch_size=cfg.batch_test, verbose=1)\n",
    "preds = probs.argmax(axis=1)\n",
    "\n",
    "print(classification_report(test_labels, preds, target_names=class_names))\n",
    "\n",
    "cm = confusion_matrix(test_labels, preds)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix'):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha='right')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(cm, class_names, normalize=False, title='Confusion Matrix')\n",
    "plot_confusion_matrix(cm, class_names, normalize=True, title='Confusion Matrix (Normalized)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f585c",
   "metadata": {},
   "source": [
    "## 10) Single-Image Inference Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562cfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(path, target_size):\n",
    "    img = Image.open(path).convert(\"RGB\").resize(target_size)\n",
    "    arr = np.array(img).astype(\"float32\") / 255.0\n",
    "    return arr\n",
    "\n",
    "def predict_single_image(model, image_path, class_names, target_size=(224,224)):\n",
    "    arr = load_and_preprocess_image(image_path, target_size)\n",
    "    x = np.expand_dims(arr, axis=0)\n",
    "    probs = model.predict(x, verbose=0)[0]\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    pred_class = class_names[pred_idx]\n",
    "    confidence = float(probs[pred_idx])\n",
    "    return pred_class, confidence, probs\n",
    "\n",
    "# Example:\n",
    "# image_path = \"datasets/val/<class>/<file>.jpg\"\n",
    "# pred_class, conf, prob = predict_single_image(best_model, image_path, class_names, target_size=cfg.image_size)\n",
    "# print(pred_class, conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1045736",
   "metadata": {},
   "source": [
    "## 11) Switching Backbone (MobileNetV2 / EfficientNetB0 / EfficientNetV2M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965281ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To switch backbone, rebuild and retrain:\n",
    "# model = build_classifier(\"mobilenetv2\", num_classes=cfg.num_classes, input_shape=cfg.image_size)\n",
    "# model = build_classifier(\"efficientnetb0\", num_classes=cfg.num_classes, input_shape=cfg.image_size)\n",
    "# model = build_classifier(\"efficientnetv2m\", num_classes=cfg.num_classes, input_shape=cfg.image_size)\n",
    "# compile_model(model, cfg.base_learning_rate)\n",
    "# history = train_model(model, train_ds, val_ds, epochs=15, callbacks=[checkpoint_cb, earlystop_cb, reducelr_cb])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
