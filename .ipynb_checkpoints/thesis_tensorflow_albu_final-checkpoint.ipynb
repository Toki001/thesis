{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995a1220",
   "metadata": {},
   "source": [
    "# TensorFlow/Keras rewrite (Full parity) with Albumentations\n",
    "\n",
    "This notebook is a line-by-line TensorFlow/Keras port of the original PyTorch notebook, preserving dataset splitting, augmentation (Albumentations), multi-stage training with progressive unfreezing, CSV logging, plotting, evaluation, single-image inference, and TFLite export. All imports are consolidated at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4f462a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Core & utilities\n",
    "import os, sys, platform, random, shutil, itertools, math\n",
    "from pathlib import Path\n",
    "\n",
    "# Data & numerics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Image handling\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Albumentations\n",
    "import albumentations as A\n",
    "import platform\n",
    "import psutil\n",
    "print(\"All imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eacd19c",
   "metadata": {},
   "source": [
    "## Environment check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326e671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SYSTEM INFO ---\n",
      "Python version: 3.12.2\n",
      "TensorFlow version: 2.17.0\n",
      "Platform: macOS-15.3.1-arm64-arm-64bit\n",
      "No GPU detected (check TensorFlow-metal plugin).\n",
      "\n",
      "--- MEMORY REPORT ---\n",
      "Total RAM:     18.00 GB\n",
      "Available RAM: 6.03 GB\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- SYSTEM INFO ---\")\n",
    "print(\"Python version:\", platform.python_version())\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Check available devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Available GPU(s):\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected (check TensorFlow-metal plugin).\")\n",
    "\n",
    "# Memory info (system-wide)\n",
    "svmem = psutil.virtual_memory()\n",
    "print(\"\\n--- MEMORY REPORT ---\")\n",
    "print(f\"Total RAM:     {svmem.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available RAM: {svmem.available / (1024**3):.2f} GB\")\n",
    "\n",
    "# VRAM usage (Apple GPUs don’t expose per-GPU memory stats like CUDA, but TF shows allocation)\n",
    "if gpus:\n",
    "    try:\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(\"\\nLogical GPU(s):\", logical_gpus)\n",
    "\n",
    "        # Create a test tensor to force GPU allocation\n",
    "        with tf.device('/GPU:0'):\n",
    "            test_tensor = tf.random.uniform([1000, 1000])\n",
    "        print(\"✅ Tensor allocated on GPU\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Could not allocate tensor on GPU:\", e)\n",
    "\n",
    "print(\"-------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90942c8",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f385741b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 2\n"
     ]
    }
   ],
   "source": [
    "# Paths (adjust to your dataset)\n",
    "source_dir = Path('/Users/jameskierdoliguez/Desktop/test_dataset/dataset')    # original un-split data (class subfolders)\n",
    "output_dir = Path('/Users/jameskierdoliguez/Desktop/test_dataset/aug_dataset')             # where train/val/test will be placed (after splitting)\n",
    "OUTPUTS = Path('/Users/jameskierdoliguez/Documents/VSCODE/thesis/test_outputs')\n",
    "OUTPUTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_dir_train = output_dir / 'train'\n",
    "output_dir_val = output_dir / 'val'\n",
    "output_dir_test = output_dir / 'test'\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "INIT_EPOCHS = 20\n",
    "SEED = 42\n",
    "\n",
    "# History CSV path\n",
    "HISTORY_CSV = OUTPUTS / 'training_history_test_v1.csv'\n",
    "print(\"done 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3df24",
   "metadata": {},
   "source": [
    "## Dataset splitting utility (prepare_image_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be544cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def prepare_image_datasets(source_dir, output_dir, split_ratios=(0.7, 0.2, 0.1), seed=SEED):\n",
    "    if not Path(source_dir).exists():\n",
    "        raise ValueError(f\"Source dir {source_dir} not found\")\n",
    "    if not math.isclose(sum(split_ratios), 1.0, abs_tol=1e-3):\n",
    "        raise ValueError(\"Split ratios must sum to 1.\")\n",
    "\n",
    "    classes = [p.name for p in Path(source_dir).iterdir() if p.is_dir()]\n",
    "    random.seed(seed)\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in classes:\n",
    "            (Path(output_dir)/split/cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls in classes:\n",
    "        files = list(itertools.chain.from_iterable(\n",
    "            Path(source_dir/cls).glob(f\"*.{ext}\") for ext in [\"jpg\", \"jpeg\", \"png\", \"bmp\"]\n",
    "        ))\n",
    "        random.shuffle(files)\n",
    "\n",
    "        n_total = len(files)\n",
    "        n_train = int(split_ratios[0] * n_total)\n",
    "        n_test  = int(split_ratios[1] * n_total)\n",
    "        n_val   = n_total - n_train - n_test\n",
    "\n",
    "        splits = {\n",
    "            \"train\": files[:n_train],\n",
    "            \"test\": files[n_train:n_train+n_test],\n",
    "            \"val\": files[n_train+n_test:]\n",
    "        }\n",
    "\n",
    "        for split, items in splits.items():\n",
    "            for f in tqdm(items, desc=f\"Copying {cls} → {split}\"):\n",
    "                shutil.copy2(f, Path(output_dir)/split/cls)\n",
    "\n",
    "    print(\"Dataset split completed.\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf65639",
   "metadata": {},
   "source": [
    "## Data loading (tf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032797ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 2\n"
     ]
    }
   ],
   "source": [
    "def load_datasets_from_dirs(train_dir=TRAIN_DIR, val_dir=VAL_DIR, test_dir=TEST_DIR, img_size=IMG_SIZE, batch_size=BATCH_SIZE):\n",
    "    train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        train_dir, image_size=img_size, batch_size=batch_size,\n",
    "        shuffle=True, seed=SEED\n",
    "    )\n",
    "    val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        val_dir, image_size=img_size, batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "        test_dir, image_size=img_size, batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_ds, val_ds, test_ds\n",
    "print(\"done 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca15f4",
   "metadata": {},
   "source": [
    "## Albumentations pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1859f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done q\n"
     ]
    }
   ],
   "source": [
    "# Training pipeline - more aggressive\n",
    "train_albu = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
    "    A.RandomResizedCrop(size=(IMG_SIZE[0], IMG_SIZE[1]), scale=(0.8, 1.0), p=0.8),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.Affine(translate_percent={\"x\": (-0.07, 0.07), \"y\": (-0.07, 0.07)},scale=(0.9, 1.1),rotate=(-25, 25),p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.HueSaturationValue(p=0.3),\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0)),\n",
    "        A.MotionBlur(blur_limit=3)], p=0.4),\n",
    "])\n",
    "\n",
    "# Validation/test pipeline - deterministic resize\n",
    "val_albu = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE[0], width=IMG_SIZE[1]),\n",
    "])\n",
    "print(\"done q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c213d",
   "metadata": {},
   "source": [
    "## Apply Albumentations inside tf.data (bridge functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d78c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def _apply_albu_to_batch(image, label, pipeline):\n",
    "    def _aug_fn(img):\n",
    "        aug = pipeline(image=img.numpy())['image']\n",
    "        return aug.astype(np.uint8)\n",
    "\n",
    "    aug_image = tf.numpy_function(_aug_fn, [image], Tout=tf.uint8)\n",
    "    aug_image.set_shape([IMG_SIZE[0], IMG_SIZE[1], 3])\n",
    "    aug_image = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
    "        tf.cast(aug_image, tf.float32)\n",
    "    )\n",
    "    return aug_image, label\n",
    "\n",
    "def apply_albumentations_to_dataset(ds, pipeline, shuffle=False):\n",
    "    ds = ds.map(lambda x, y: _apply_albu_to_batch(x, y, pipeline),\n",
    "                num_parallel_calls=AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "    return ds.prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c2e0b",
   "metadata": {},
   "source": [
    "## Visualize samples (after augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68031481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def show_batch(ds, class_names, n=9):\n",
    "    imgs, labels = next(iter(ds))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(3, 3, i+1)\n",
    "        img = (imgs[i].numpy() + 1) / 2  # de-normalize\n",
    "        plt.imshow(np.clip(img, 0, 1))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e4f979",
   "metadata": {},
   "source": [
    "## Model builders (MobileNetV2 & EfficientNetV2M) and unfreeze helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce9c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def build_mobilenetv2(num_classes, img_size=IMG_SIZE):\n",
    "    inputs = keras.Input(shape=(*img_size, 3))\n",
    "    base = keras.applications.MobileNetV2(input_shape=(*img_size, 3),\n",
    "                                          include_top=False,\n",
    "                                          weights=\"imagenet\")\n",
    "    base.trainable = False\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs), base\n",
    "\n",
    "def unfreeze_last_n_layers(base_model, n):\n",
    "    if n > 0:\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:-n]:\n",
    "            layer.trainable = False\n",
    "        print(f\"Unfroze last {n} layers of {len(base_model.layers)}\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3896ee-63c7-4a38-adae-a6a48cfb943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnetv2m(num_classes, img_size=IMG_SIZE):\n",
    "    inputs = keras.Input(shape=(*img_size, 3))\n",
    "    base = keras.applications.EfficientNetV2M(input_shape=(*img_size, 3),\n",
    "                                              include_top=False,\n",
    "                                              weights=\"imagenet\")\n",
    "    base.trainable = False\n",
    "    x = base(inputs, training=False)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs), base\n",
    "\n",
    "def unfreeze_last_n_layers(base_model, n):\n",
    "    if n > 0:\n",
    "        base_model.trainable = True\n",
    "        for layer in base_model.layers[:-n]:\n",
    "            layer.trainable = False\n",
    "        print(f\"Unfroze last {n} layers of {len(base_model.layers)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c79329",
   "metadata": {},
   "source": [
    "## Compile & Callbacks utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea0127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 1\n"
     ]
    }
   ],
   "source": [
    "def compile_model(model, lr=1e-3):\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "def make_callbacks(name_prefix=\"model\"):\n",
    "    return [\n",
    "        EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-7, verbose=1),\n",
    "        ModelCheckpoint(OUTPUTS/f\"{name_prefix}_best.keras\", monitor=\"val_accuracy\",\n",
    "                        save_best_only=True, verbose=1)\n",
    "    ]\n",
    "print(\"done 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90e614",
   "metadata": {},
   "source": [
    "## History CSV logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "381d4043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def append_history(history, csv_path=HISTORY_CSV):\n",
    "    df = pd.DataFrame(history.history)\n",
    "    df['epoch'] = np.arange(len(df)) + 1\n",
    "\n",
    "    if csv_path.exists():\n",
    "        old = pd.read_csv(csv_path)\n",
    "        start = old['epoch'].max() + 1\n",
    "        df['epoch'] = np.arange(start, start+len(df))\n",
    "        df = pd.concat([old, df], ignore_index=True)\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Appended history to {csv_path}\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f449a",
   "metadata": {},
   "source": [
    "## Incremental training (multi-session unfreezing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e7376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def incremental_training(model, base_model, train_ds, val_ds, sessions, prefix=\"stage\"):\n",
    "    histories = []\n",
    "    for i, cfg in enumerate(sessions, 1):\n",
    "        print(f\"--- Session {i}: {cfg} ---\")\n",
    "        if \"unfreeze_layer\" in cfg:\n",
    "            unfreeze_last_n_layers(base_model, cfg[\"unfreeze_layer\"])\n",
    "        compile_model(model, lr=cfg[\"lr\"])\n",
    "        h = model.fit(train_ds,\n",
    "                      validation_data=val_ds,\n",
    "                      epochs=cfg[\"epochs\"],\n",
    "                      callbacks=make_callbacks(f\"{prefix}_s{i}\"))\n",
    "        append_history(h)\n",
    "        histories.append(h.history)\n",
    "    return histories\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce204e49",
   "metadata": {},
   "source": [
    "## Plot utilities (session & historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fcc5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def plot_sessions(*histories):\n",
    "    loss, val_loss, acc = [], [], []\n",
    "    for h in histories:\n",
    "        loss.extend(h[\"loss\"])\n",
    "        val_loss.extend(h[\"val_loss\"])\n",
    "        acc.extend(h[\"accuracy\"])\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(loss, label=\"loss\"); plt.plot(val_loss, label=\"val_loss\")\n",
    "    plt.legend(); plt.title(\"Loss\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(acc, label=\"accuracy\")\n",
    "    plt.legend(); plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_history_csv(csv_path=HISTORY_CSV):\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        sns.lineplot(df, x=\"epoch\", y=\"loss\", label=\"loss\")\n",
    "        sns.lineplot(df, x=\"epoch\", y=\"val_loss\", label=\"val_loss\")\n",
    "        plt.show()\n",
    "        if \"accuracy\" in df:\n",
    "            sns.lineplot(df, x=\"epoch\", y=\"accuracy\", label=\"acc\")\n",
    "            sns.lineplot(df, x=\"epoch\", y=\"val_accuracy\", label=\"val_acc\")\n",
    "            plt.show()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955ce9a",
   "metadata": {},
   "source": [
    "## Build datasets and preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84129082",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory /Users/jameskierdoliguez/Desktop/test_dataset/aug_dataset/train",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Optionally split raw dataset first (uncomment if needed)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# prepare_image_datasets(INPUT_RAW_DIR, DATA_DIR, split_ratios=(0.7,0.2,0.1), seed=SEED)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load datasets from DATA_DIR directories (expects train/val/test present)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_ds_raw, val_ds_raw, test_ds_raw \u001b[38;5;241m=\u001b[39m load_datasets_from_dirs()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Apply Albumentations: aggressive to train, deterministic to val/test\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m apply_albumentations_to_dataset(train_ds_raw, train_albu, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m, in \u001b[0;36mload_datasets_from_dirs\u001b[0;34m(train_dir, val_dir, test_dir, img_size, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_datasets_from_dirs\u001b[39m(train_dir\u001b[38;5;241m=\u001b[39mTRAIN_DIR, val_dir\u001b[38;5;241m=\u001b[39mVAL_DIR, test_dir\u001b[38;5;241m=\u001b[39mTEST_DIR, img_size\u001b[38;5;241m=\u001b[39mIMG_SIZE, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE):\n\u001b[0;32m----> 2\u001b[0m     train_ds \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m      3\u001b[0m         train_dir, image_size\u001b[38;5;241m=\u001b[39mimg_size, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      4\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39mSEED\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     val_ds \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m      7\u001b[0m         val_dir, image_size\u001b[38;5;241m=\u001b[39mimg_size, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      8\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     test_ds \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     11\u001b[0m         test_dir, image_size\u001b[38;5;241m=\u001b[39mimg_size, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m     12\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/image_dataset_utils.py:232\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[0;32m--> 232\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mindex_directory(\n\u001b[1;32m    233\u001b[0m     directory,\n\u001b[1;32m    234\u001b[0m     labels,\n\u001b[1;32m    235\u001b[0m     formats\u001b[38;5;241m=\u001b[39mALLOWLIST_FORMATS,\n\u001b[1;32m    236\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclass_names,\n\u001b[1;32m    237\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[1;32m    238\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    239\u001b[0m     follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[1;32m    240\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mlistdir(directory)):\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/lib/io/file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \n\u001b[1;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[0;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[1;32m    778\u001b[0m ]\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /Users/jameskierdoliguez/Desktop/test_dataset/aug_dataset/train"
     ]
    }
   ],
   "source": [
    "# Optionally split raw dataset first (uncomment if needed)\n",
    "# prepare_image_datasets(INPUT_RAW_DIR, DATA_DIR, split_ratios=(0.7,0.2,0.1), seed=SEED)\n",
    "\n",
    "# Load datasets from DATA_DIR directories (expects train/val/test present)\n",
    "train_ds_raw, val_ds_raw, test_ds_raw = load_datasets_from_dirs()\n",
    "\n",
    "# Apply Albumentations: aggressive to train, deterministic to val/test\n",
    "train_ds = apply_albumentations_to_dataset(train_ds_raw, train_albu, shuffle=True)\n",
    "val_ds = apply_albumentations_to_dataset(val_ds_raw, val_albu, shuffle=False)\n",
    "test_ds = apply_albumentations_to_dataset(test_ds_raw, val_albu, shuffle=False)\n",
    "\n",
    "# Class names\n",
    "class_names = train_ds_raw.class_names\n",
    "num_classes = len(class_names)\n",
    "print('Classes:', class_names)\n",
    "\n",
    "# Visual check\n",
    "show_batch(train_ds, class_names)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53786b04",
   "metadata": {},
   "source": [
    "## Build model (MobileNetV2) and initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, base = build_mobilenetv2(num_classes)\n",
    "compile_model(model, lr=1e-3)\n",
    "hist0 = model.fit(train_ds,\n",
    "                  validation_data=val_ds,\n",
    "                  epochs=INIT_EPOCHS,\n",
    "                  callbacks=make_callbacks(\"base\"))\n",
    "append_history(hist0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2b1c48",
   "metadata": {},
   "source": [
    "## Incremental sessions example (edit as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [\n",
    "    {\"epochs\": 10, \"lr\": 5e-4, \"unfreeze_layer\": 0},\n",
    "    {\"epochs\": 15, \"lr\": 1e-4, \"unfreeze_layer\": 10},\n",
    "    {\"epochs\": 10, \"lr\": 5e-5, \"unfreeze_layer\": 30},\n",
    "]\n",
    "histories = incremental_training(model, base, train_ds, val_ds, sessions, prefix=\"mobilenetv2\")\n",
    "plot_sessions(hist0.history, *histories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17672548",
   "metadata": {},
   "source": [
    "## Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, ds, class_names, out_csv=OUTPUTS/\"predictions.csv\"):\n",
    "    y_true, y_pred = [], []\n",
    "    for imgs, labels in ds:\n",
    "        preds = model.predict(imgs)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names, cmap=\"Blues\")\n",
    "    plt.show()\n",
    "    pd.DataFrame({\"true\": y_true, \"pred\": y_pred}).to_csv(out_csv, index=False)\n",
    "    print(f\"Predictions saved to {out_csv}\")\n",
    "\n",
    "evaluate_and_save(model, test_ds, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a47174",
   "metadata": {},
   "source": [
    "## Single-image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, path, class_names):\n",
    "    img = Image.open(path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    arr = np.array(img)\n",
    "    aug = val_albu(image=arr)[\"image\"]\n",
    "    arr = tf.keras.applications.mobilenet_v2.preprocess_input(aug.astype(np.float32))\n",
    "    arr = np.expand_dims(arr, 0)\n",
    "    preds = model.predict(arr)\n",
    "    idx = np.argmax(preds)\n",
    "    print(\"Pred:\", class_names[idx], \" (conf:\", preds[0][idx], \")\")\n",
    "    plt.imshow(img); plt.title(class_names[idx]); plt.axis(\"off\"); plt.show()\n",
    "\n",
    "example_img = list(TEST_DIR.rglob(\"*.jpg\"))[0]\n",
    "predict_single_image(model, example_img, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c942d7d",
   "metadata": {},
   "source": [
    "## Save model & TFLite conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090ca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = OUTPUTS / 'saved_model_final'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "model.save(save_dir, include_optimizer=False)\n",
    "print('Saved model to', save_dir)\n",
    "\n",
    "# TFLite float16 conversion\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(save_dir))\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "tflite_path = OUTPUTS / 'model_float16.tflite'\n",
    "tflite_path.write_bytes(tflite_model)\n",
    "print('Wrote TFLite model to', tflite_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Toki)",
   "language": "python",
   "name": "toki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
