{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e72b406c",
   "metadata": {},
   "source": [
    "# Implementation of Mobile-based CNN (TensorFlow/Keras) for Detecting Rice Leaf Diseases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13765faa",
   "metadata": {},
   "source": [
    "This notebook is a **full TensorFlow/Keras rewrite** of the original PyTorch notebook.\n",
    "It covers: data loading with `tf.data`, transfer learning (MobileNetV2/EfficientNet), training,\n",
    "evaluation with confusion matrix, and **TFLite conversion** for mobile deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python\n",
    "import sys, platform, itertools\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical & Data Handling\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow / Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Metrics / Evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Image Handling\n",
    "from PIL import Image\n",
    "\n",
    "# Albumentations\n",
    "import albumentations as A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0144240a",
   "metadata": {},
   "source": [
    "## 2) Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ae560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"GPU(s):\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1cd2c9",
   "metadata": {},
   "source": [
    "## 3) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c6750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = Path(\"data\")   # Change this to your dataset root folder\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "VAL_SPLIT = 0.2\n",
    "SEED = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736e7d3",
   "metadata": {},
   "source": [
    "## 4) Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171857d2",
   "metadata": {},
   "source": [
    "## 5) Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Albumentations pipeline\n",
    "albumentations_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.RandomResizedCrop(height=IMG_SIZE[0], width=IMG_SIZE[1], scale=(0.8, 1.0), p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8366c",
   "metadata": {},
   "source": [
    "## 6) Apply Albumentations in tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc768de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def albumentations_augment(image, label):\n",
    "    image = image.numpy()\n",
    "    augmented = albumentations_transform(image=image)\n",
    "    image = augmented[\"image\"]\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "def augment_with_albumentations(image, label):\n",
    "    aug_img, aug_label = tf.py_function(\n",
    "        albumentations_augment, [image, label], [tf.float32, label.dtype]\n",
    "    )\n",
    "    aug_img.set_shape((IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "    return aug_img, aug_label\n",
    "\n",
    "# Apply only to training dataset\n",
    "train_ds = train_ds.map(augment_with_albumentations, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bab548",
   "metadata": {},
   "source": [
    "## 7) Visualize Augmented Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12417f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_ds.take(1):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(min(9, images.shape[0])):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[int(labels[i])])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4e751b",
   "metadata": {},
   "source": [
    "## 8) Model (MobileNetV2 Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d59204",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "base = keras.applications.MobileNetV2(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "base.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=INPUT_SHAPE)\n",
    "x = keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"riceleaf_mobilenetv2\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04481a21",
   "metadata": {},
   "source": [
    "## 9) Compile & Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "checkpoint_path = str((OUTPUT_DIR / \"best_model.keras\").resolve())\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, min_lr=1e-6, verbose=1),\n",
    "    ModelCheckpoint(checkpoint_path, monitor=\"val_accuracy\", save_best_only=True, verbose=1),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c29b6",
   "metadata": {},
   "source": [
    "## 8) Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f58343",
   "metadata": {},
   "source": [
    "## 10) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc907505",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8f3be",
   "metadata": {},
   "source": [
    "## 11) Plot Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history.get(\"accuracy\", [])\n",
    "val_acc = history.history.get(\"val_accuracy\", [])\n",
    "loss = history.history.get(\"loss\", [])\n",
    "val_loss = history.history.get(\"val_loss\", [])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(acc, label=\"train_acc\")\n",
    "plt.plot(val_acc, label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dcbde",
   "metadata": {},
   "source": [
    "## 13) Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect predictions on val_ds\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for batch_imgs, batch_labels in val_ds:\n",
    "    preds = model.predict(batch_imgs, verbose=0)\n",
    "    y_true.extend(batch_labels.numpy().tolist())\n",
    "    y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Plot CM\n",
    "import itertools\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1fca3",
   "metadata": {},
   "source": [
    "## 13) Save Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e625878",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = OUTPUT_DIR / \"saved_model\"\n",
    "saved_model_dir.mkdir(exist_ok=True, parents=True)\n",
    "model.save(saved_model_dir, include_optimizer=False)\n",
    "print(\"SavedModel ->\", saved_model_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ec86e",
   "metadata": {},
   "source": [
    "## 14) TFLite Conversion (Float16 & INT8 options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_float16 = OUTPUT_DIR / \"model_float16.tflite\"\n",
    "tflite_int8 = OUTPUT_DIR / \"model_int8.tflite\"\n",
    "\n",
    "# Float16 quantization (good trade-off for mobile GPUs/NPUs)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "tflite_float16.write_bytes(tflite_model)\n",
    "print(\"Wrote:\", tflite_float16.resolve())\n",
    "\n",
    "# Full INT8 quantization (requires a calibration dataset)\n",
    "def representative_data_gen():\n",
    "    for images, _ in train_ds.take(50):  # 50 batches for calibration\n",
    "        yield [tf.cast(images, tf.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_int8_model = converter.convert()\n",
    "tflite_int8.write_bytes(tflite_int8_model)\n",
    "print(\"Wrote:\", tflite_int8.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9302b",
   "metadata": {},
   "source": [
    "## 15) Single Image Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdea9317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, target_size=IMG_SIZE):\n",
    "    img = Image.open(path).convert(\"RGB\").resize(target_size, Image.BILINEAR)\n",
    "    arr = np.array(img, dtype=np.float32)\n",
    "    arr = tf.keras.applications.mobilenet_v2.preprocess_input(arr)\n",
    "    return np.expand_dims(arr, 0)\n",
    "\n",
    "# Example:\n",
    "# test_path = DATA_DIR / class_names[0] / \"some_image.jpg\"\n",
    "# x = load_image(test_path)\n",
    "# preds = model.predict(x)\n",
    "# print(\"Pred:\", class_names[int(np.argmax(preds))], preds.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e62cbd",
   "metadata": {},
   "source": [
    "## 16) Notes on Porting from PyTorch → TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3cc08",
   "metadata": {},
   "source": [
    "- `torch.utils.data.Dataset/DataLoader` → `tf.data` via `image_dataset_from_directory`.\n",
    "- Manual training loops (`optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`) → `model.fit()` with callbacks.\n",
    "- `torchvision.models` (e.g., ResNet, MobileNet) → `tf.keras.applications` equivalents.\n",
    "- Saving: `torch.save(state_dict)` → `model.save(SavedModel)` and TFLite export.\n",
    "- Device: `with torch.cuda.amp.autocast()` → mixed precision via `tf.keras.mixed_precision` (optional).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Toki)",
   "language": "python",
   "name": "toki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
